---
title: "Project 2"
author: "Brian Krumholz"
date: "3/4/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir ="C:/Users/Lugal/OneDrive/Documents/MSBA/TBANLT 560/Project 2/TBANLT560_PROJ2/")
```

Purpose: This project is built to explore if an ensemble model for classification can provide an more accurate model for classifying data. This project will be using the BreastCancer dataset from the mlbench library. The data was collected by Dr. WOlberg at the University of Wisconsin.

```{r libraries}
library(tidyverse)
library(mlbench)
library(e1071)
library(klaR)
library(nnet)
library(neuralnet)
library(MASS)
library(rpart)
library(randomForest)
library(caret)
library(plyr)
library(gplots)
library(ggplot2)
```

```{r}

data(BreastCancer)
df<-BreastCancer
summary(df)
```

The models will predict if a cell sample is malignant or benign. To help with the analyze, the "Class" column will be converted to a binary category called "malignant" where a value of 1 represents a malignant sample and a value of 0 indicates that it is benign. This conversion is done as some models need a numerical response variable.

As part of the data cleaning process, rows that have NA values will be dropped. While some models like decision trees can handle NAs, other models cannot. Since the purpose is to combine the results from all models, the data used must be useable by all the models.

```{r var_cleaning}

df<-na.omit(df)  #Get rid of rows with NAs
df$malignant<-as.factor(ifelse(df$Class=="malignant",1,0))  #Convert Class into binary
df<-within(df,rm(Class,Id))
```


To start the analysis process, the cell data will be broken into a training partition and a validation partition. The training partition which includes 60% of records will be used to train each model. The same training records will be used for each model to minimize random chance giving a model an edge in predictions. The remaining 40% of the data will be used to evaluate how well the model does with data that it hasn't been trained on. 

```{r setup_partition}
dim(df)
set.seed(1)
train_ind<-sample(dim(df)[1],dim(df)[1]*.6)
train.df<-df[train_ind,]
valid.df<-df[-train_ind,]
```

Two data frame are created that will store information about each model. The model.stats data frame will record each model's accuracy with the training data and the validation data. The model.results data frame will store the predicted values for each record of the data. Each column will be the predicted values of one model.

```{r model_stats}
  model.stats<-data.frame(Model=character(),Training.Accuracy=double(),Validation.Accuracy=double()) #dataframe to store statistics about model
  model.results<-as.data.frame(as.numeric(as.character(df$malignant))) #putting the actual result for each sample
  colnames(model.results)<-"Actual.Malignant"
  df.len<-dim(model.results)[1]  #creating an empty dataframe to store results of model
  model.results$svm<-rep(0,df.len)
  model.results$nb<-rep(0,df.len)
  model.results$nn<-rep(0,df.len)
  model.results$dt<-rep(0,df.len)
  model.results$cv<-rep(0,df.len)
  model.results$qda<-rep(0,df.len)
  model.results$rda<-rep(0,df.len)
  model.results$rf<-rep(0,df.len)
```

The first model to run against the data is the support vector machine algorithm or SVM for short. This algorithm works by creating hyperplane in n-dimensional space that will separate the benign and malignant records. With our 9 predictors, the hyperplane will be an 8 dimension plane. While this is hard to visualize, the algorithm is not very computationally expensive to run.

```{r supportvectormachine_training}
mysvm <- svm(malignant ~ ., train.df)
mysvm.pred <- predict(mysvm, train.df)
table(mysvm.pred,train.df$malignant)
train.svm.cm<-confusionMatrix(mysvm.pred,train.df$malignant,positive = "1")
train.svm.cm

```

Checking the training data predictions of the SVM model, we get a 97.6% accuracy. Now we will see if the model is as good with the validation data set.

```{r supportvectormachine_valid}
mysvm.pred <- predict(mysvm, valid.df)
valid.svm.cm<-confusionMatrix(mysvm.pred,valid.df$malignant,positive = "1")
valid.svm.cm
model.stats[1,]=c("Support Vector Machine",train.svm.cm$overall[1],valid.svm.cm$overall[1])
```

The validation data set predictions are slightly worse than the training with 96.0% accuracy. That indicates there is probably a slight bit of overfitting, but the model still gives good results with new data.

Now the SVM model will be run for the full data set and the results will be saved off to build the ensemble model.

```{r svm_full_predict}
mysvm.pred <- predict(mysvm, df)
model.results$svm<-as.numeric(as.character(mysvm.pred))
```


```{r naiveBayes_train}
mynb <- naiveBayes(malignant ~ ., train.df)
mynb.pred <- predict(mynb, train.df)
train.nb.cm<-confusionMatrix(mynb.pred,train.df$malignant,positive="1")
train.nb.cm
```

```{r naiveBayes_validation}
mynb.pred <- predict(mynb, valid.df)
table(mynb.pred,valid.df$malignant)
valid.nb.cm<-confusionMatrix(mynb.pred,valid.df$malignant,positive="1")
valid.nb.cm
model.stats[2,]=c("NaiveBayes",train.nb.cm$overall[1],valid.nb.cm$overall[1])
```

```{r naiveBayes_full_predict}
mynb.pred <- predict(mynb, df)
model.results$nb<-as.numeric(as.character(mynb.pred))
```


```{r neuralnet_setup}
df2<-df
#convert to integers #There are better ways to do this
df2$Cl.thickness<-as.integer(df$Cl.thickness)
df2$malignant<-ifelse(as.integer(df$malignant)==2,1,0)
df2$Mitoses<-as.integer(df$Mitoses)
df2$Cell.size<-as.integer(df$Cell.size)
df2$Cell.shape<-as.integer(df$Cell.shape)
df2$Marg.adhesion<-as.integer(df$Marg.adhesion)
df2$Epith.c.size<-as.integer(df$Epith.c.size)
df2$Bare.nuclei<-as.integer(df$Bare.nuclei)
df2$Bl.cromatin<-as.integer(df$Bl.cromatin)
df2$Normal.nucleoli<-as.integer(df$Normal.nucleoli)

# BreastCancerqda <- lapply(BreastCancer,as.numeric)
# BreastCancerqda$Class <- factor(BreastCancerqda$Class, labels = c("benign", "malignant"))


preproc<-preProcess(df2,method = "range")
df.range<-predict(preproc,df2)

train.df.range<-df.range[train_ind,]
valid.df.range<-df.range[-train_ind,]
```


```{r neuralnet_training}
mynn <- neuralnet::neuralnet(malignant ~ ., data=train.df.range,linear.output=T,hidden=c(5),rep=10)
plot(mynn)
mynn.pred <- ifelse(predict(mynn, newdata=train.df.range)>.5,1,0)
train.nn.cm<-confusionMatrix(as.factor(mynn.pred),as.factor(train.df.range$malignant),positive = "1")
train.nn.cm
```

```{r neuralnet_valid}
mynn.pred <- ifelse(predict(mynn, newdata=valid.df.range)>.5,1,0)
valid.nn.cm<-confusionMatrix(as.factor(mynn.pred),as.factor(valid.df.range$malignant),positive = "1")
valid.nn.cm
model.stats[3,]=c("Neural Network",train.nn.cm$overall[1],valid.nn.cm$overall[1])
```

```{r neuralnet_full_predict}
mynn.pred <- ifelse(predict(mynn, newdata=df.range)>.5,1,0)
model.results$nn<-as.numeric(as.character(mynn.pred))
```


```{r decision_tree_training}
mytree <- rpart(malignant ~ ., train.df)
plot(mytree); text(mytree) 
mytree.pred <- predict(mytree,newdata=train.df,type="class")
train.dt.cm<-confusionMatrix(as.factor(mytree.pred),as.factor(train.df$malignant),positive = "1")
train.dt.cm
```

```{r decision_tree_validation}
mytree.pred <- predict(mytree,newdata=valid.df,type="class")
valid.dt.cm<-confusionMatrix(as.factor(mytree.pred),as.factor(valid.df$malignant),positive = "1")
valid.dt.cm
model.stats[4,]=c("Decision Tree",train.dt.cm$overall[1],valid.dt.cm$overall[1])
```

```{r}
mytree.pred <- predict(mytree,newdata=df,type="class")
model.results$dt<-as.numeric(as.character(mytree.pred))
```


Cross-Validation decision tree method doesn't have an easy way to split data into training and validation. Because of this, the full dataset is used rather than the model being run one for training and once for validation datasets.

```{r crossValidation}
ans <- numeric(length(df[,1]))
for (i in 1:length(df[,1])) {
  mytree <- rpart(malignant ~ ., df[-i,])
  mytree.pred <- predict(mytree,df[i,],type="class")
  ans[i] <- mytree.pred
}
ans <- factor(ans,labels=levels(df$malignant))
table(ans,df$malignant)
sv.cm<-confusionMatrix(as.factor(ans),as.factor(df$malignant),positive = "1")
sv.cm
model.stats[5,]=c("Cross Validation","",sv.cm$overall[1])

model.results$cv<-as.numeric(as.character(ans))
```


```{r QDA_training}
#Quadratic Discriminant Analysis

train.df2<-df2[train_ind,]
valid.df2<-df2[-train_ind,]

myqda <- qda(malignant ~ ., data=train.df2)
myqda.pred <- predict(myqda, train.df2)
# table(myqda.pred$class,training.df2$malignant)
train.qda.cm<-confusionMatrix(as.factor(myqda.pred$class),as.factor(train.df2$malignant),positive = "1")
train.qda.cm
```

```{r QDA_validation}
#Quadratic Discriminant Analysis

myqda.pred <- predict(myqda, valid.df2)
# table(myqda.pred$class,training.df2$malignant)
valid.qda.cm<-confusionMatrix(as.factor(myqda.pred$class),as.factor(valid.df2$malignant),positive = "1")
valid.qda.cm
model.stats[6,]=c("Quadratic Discriminant Analysis",train.qda.cm$overall[1],valid.qda.cm$overall[1])
```

```{r}
myqda.pred <- predict(myqda, df2)
model.results$qda<-as.numeric(as.character(myqda.pred$class))
```


```{r RDA_training}
#Regularised Discriminant Analysis
myrda <- rda(malignant ~ ., train.df2)
myrda.pred <- predict(myrda, train.df2)
train.rda.cm<-confusionMatrix(as.factor(myrda.pred$class),as.factor(train.df2$malignant),positive = "1")
train.rda.cm
```

```{r RDA_valid}
#Regularised Discriminant Analysis
myrda.pred <- predict(myrda, valid.df2)
valid.rda.cm<-confusionMatrix(as.factor(myrda.pred$class),as.factor(valid.df2$malignant),positive = "1")
valid.rda.cm
model.stats[7,]=c("Regularised Discriminant Analysis",train.rda.cm$overall[1],valid.rda.cm$overall[1])
```

```{r}
myrda.pred <- predict(myrda, df2)
model.results$rda<-as.numeric(as.character(myrda.pred$class))
```


```{r rf_training}
#Random Forests
myrf <- randomForest(malignant ~ .,train.df)
myrf.pred <- predict(myrf, train.df)
train.rf.cm<-confusionMatrix(as.factor(myrf.pred),as.factor(train.df$malignant),positive = "1")
train.rf.cm
```

```{r rf_valid}
#Random Forests
myrf.pred <- predict(myrf, valid.df)
valid.rf.cm<-confusionMatrix(as.factor(myrf.pred),as.factor(valid.df$malignant),positive = "1")
valid.rf.cm
myrf.pred.comp <- predict(myrf, df)
complete.rf.cm<-confusionMatrix(as.factor(myrf.pred.comp),as.factor(df$malignant),positive = "1")
complete.rf.cm
model.stats[8,]=c("Random Forest",train.rf.cm$overall[1],valid.rf.cm$overall[1])
```

```{r}
myrf.pred <- predict(myrf, df)
model.results$rf<-as.numeric(as.character(myrf.pred))
```

```{r calc_ensemble}
print(model.stats)
model.results$Row.Sum<-rowSums(model.results[,c(2,3,4,5,6,7,8,9)])
model.results$ensemble.pick<-ifelse(model.results$Row.Sum>4,1,ifelse(model.results$Row.Sum==4,model.results$rf,0))
```

```{r review_models}
ensemble.cm<-confusionMatrix(as.factor(model.results$ensemble.pick[-train_ind]),as.factor(model.results$Actual.Malignant[-train_ind]))
ensemble.cm
model.stats[9,]<-c("Ensemble Model","",ensemble.cm$overall[1])
model.stats$Misclassification.Count<-round((1-as.numeric(model.stats$Validation.Accuracy))*dim(valid.df)[1])
print(model.stats[order(model.stats$Validation.Accuracy,decreasing = TRUE),])

```

```{r}
cer.cor<-cor(model.results[c(2:9,11)])
plot(cer.cor)
heatmap.2(cer.cor, Rowv = FALSE, Colv = FALSE, dendrogram = "none", 
          cellnote = round(cer.cor,2), 
          notecol = "black", key = TRUE, trace = 'none', margins = c(5,5))
```

```{r decision}
correct.df<-as.data.frame(ifelse(model.results[,c(2:9,11)]==model.results$Actual.Malignant,1,0))
selected.df<-as.data.frame(ifelse(model.results[,c(2:9)]==model.results$ensemble.pick,1,0))
selected.df$correct<-ifelse(model.results$Actual.Malignant==model.results$ensemble.pick,"Yes","No")
selected.df$decision.cnt<-rowSums(selected.df[c(1:8)])
ggplot(data=selected.df,aes(x=decision.cnt,fill=correct))+geom_histogram(bins=5)+scale_x_continuous("Models in Agreement")+ guides(fill=guide_legend(title="Correct Classification"))
ggplot(data=selected.df,aes(x=decision.cnt,fill=factor(correct)))+geom_histogram(bins=5,position="fill")+scale_x_continuous("Models in Agreement")+ guides(fill=guide_legend(title="Correct Classification"))
```

```{r wrong_pick_analysis}

wrong.cnt<-dim(correct.df[correct.df$ensemble.pick==0,])[1]
wrong.df<-as.data.frame(colSums(correct.df[correct.df$ensemble.pick==0,c(1:8)]))
colnames(wrong.df)<-"Correct Count"
wrong.df$'Percent Correct'<-round(wrong.df$`Correct Count`/wrong.cnt,2)
print(wrong.df)
```

When we look at which model consistently vote wrong in the misclassified set, we see that the decision tree never goes against the ensemble to provide the correct answer. Let's drop the decision tree from the ensemble and see if that can increase the ensemble's accuracy. As an added bonus, this will make the number of models polled an odd number so there will no longer be a need for tie-breakers.

```{r calc_ensemble_2}
model.results$Row.Sum.B<-rowSums(model.results[,c(2,3,4,6,7,8,9)])
model.results$ensemble.pick.B<-ifelse(model.results$Row.Sum.B>=4,1,0)
ensemble.B.cm<-confusionMatrix(as.factor(model.results$ensemble.pick.B[-train_ind]),as.factor(model.results$Actual.Malignant[-train_ind]))
ensemble.B.cm
model.stats[10,]<-c("Ensemble Model B","",ensemble.B.cm$overall[1],round((1-as.numeric(ensemble.B.cm$overall[1]))*dim(valid.df)[1]))
print(model.stats[order(model.stats$Validation.Accuracy,decreasing = TRUE),])
```

The decision tree model was the least accurate models in the ensemble and it never went against the consensus pick for misclassifications. When it was dropped from the ensemble model, it boosted the accuracy from 0.971 to 0.974. That is the difference of one additional correct classification.