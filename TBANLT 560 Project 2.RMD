---
title: "Project 2"
author: "Brian Krumholz"
date: "3/4/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir ="C:/Users/Lugal/OneDrive/Documents/MSBA/TBANLT 560/Project 2/TBANLT560_PROJ2/")
```

Purpose: This project is built to explore if an ensemble model for classification can provide an more accurate model for classifying data. This project will be using the BreastCancer dataset from the mlbench library. The data was collected by Dr. WOlberg at the University of Wisconsin.

```{r libraries}
library(tidyverse)
library(mlbench)
library(e1071)
library(klaR)
library(nnet)
library(neuralnet)
library(MASS)
library(rpart)
library(randomForest)
library(caret)
library(plyr)
library(gplots)
library(ggplot2)
```

```{r}

data(BreastCancer)
df<-BreastCancer
summary(df)
```

The models will predict if a cell sample is malignant or benign. To help with the analyze, the "Class" column will be converted to a binary category called "malignant" where a value of 1 represents a malignant sample and a value of 0 indicates that it is benign. This conversion is done as some models need a numerical response variable.

As part of the data cleaning process, rows that have NA values will be dropped. While some models like decision trees can handle NAs, other models cannot. Since the purpose is to combine the results from all models, the data used must be useable by all the models.

```{r var_cleaning}

df<-na.omit(df)  #Get rid of rows with NAs
df$malignant<-as.factor(ifelse(df$Class=="malignant",1,0))  #Convert Class into binary
df<-within(df,rm(Class,Id))
```


To start the analysis process, the cell data will be broken into a training partition and a validation partition. The training partition which includes 60% of records will be used to train each model. The same training records will be used for each model to minimize random chance giving a model an edge in predictions. The remaining 40% of the data will be used to evaluate how well the model does with data that it hasn't been trained on. 

```{r setup_partition}
dim(df)
set.seed(1)
train_ind<-sample(dim(df)[1],dim(df)[1]*.6)
train.df<-df[train_ind,]
valid.df<-df[-train_ind,]
```

Two data frame are created that will store information about each model. The model.stats data frame will record each model's accuracy with the training data and the validation data. The model.results data frame will store the predicted values for each record of the data. Each column will be the predicted values of one model.

```{r model_stats}
  model.stats<-data.frame(Model=character(),Training.Accuracy=double(),Validation.Accuracy=double()) #dataframe to store statistics about model
  model.results<-as.data.frame(as.numeric(as.character(df$malignant))) #putting the actual result for each sample
  colnames(model.results)<-"Actual.Malignant"
  df.len<-dim(model.results)[1]  #creating an empty dataframe to store results of model
  model.results$svm<-rep(0,df.len)
  model.results$nb<-rep(0,df.len)
  model.results$nn<-rep(0,df.len)
  model.results$dt<-rep(0,df.len)
  model.results$cv<-rep(0,df.len)
  model.results$qda<-rep(0,df.len)
  model.results$rda<-rep(0,df.len)
  model.results$rf<-rep(0,df.len)
```

The first model to run against the data is the support vector machine algorithm or SVM for short. This algorithm works by creating hyperplane in n-dimensional space that will separate the benign and malignant records. With our 9 predictors, the hyperplane will be an 8 dimension plane. While this is hard to visualize, the algorithm is not very computationally expensive to run.

```{r supportvectormachine_training}
mysvm <- svm(malignant ~ ., train.df)
mysvm.pred <- predict(mysvm, train.df)
table(mysvm.pred,train.df$malignant)
train.svm.cm<-confusionMatrix(mysvm.pred,train.df$malignant,positive = "1")
train.svm.cm

```

Checking the training data predictions of the SVM model, we get a 97.6% accuracy. Now we will see if the model is as good with the validation data set.

```{r supportvectormachine_valid}
mysvm.pred <- predict(mysvm, valid.df)
valid.svm.cm<-confusionMatrix(mysvm.pred,valid.df$malignant,positive = "1")
valid.svm.cm
model.stats[1,]=c("Support Vector Machine",train.svm.cm$overall[1],valid.svm.cm$overall[1])
```

The validation data set predictions are slightly worse than the training with 96.0% accuracy. That indicates there is probably a slight bit of overfitting, but the model still gives good results with new data.

Now the SVM model will be run for the full data set and the results will be saved off to build the ensemble model.

```{r svm_full_predict}
mysvm.pred <- predict(mysvm, df) #run svm model on full data set
model.results$svm<-as.numeric(as.character(mysvm.pred)) #store results in model.results for future analysis
```

The next model is based on the Naive Bayes algorithm. Naive Bayes is built on the concept of conditional probability. The algorithm identifies the most likely class based on the probabilities of each predictor for a given response class.

```{r naiveBayes_train}
mynb <- naiveBayes(malignant ~ ., train.df)
mynb.pred <- predict(mynb, train.df)
train.nb.cm<-confusionMatrix(mynb.pred,train.df$malignant,positive="1")
train.nb.cm
```

The training data set predictions for the Naive Bayes given strong results with 97.8% accuracy.

```{r naiveBayes_validation}
mynb.pred <- predict(mynb, valid.df)
table(mynb.pred,valid.df$malignant)
valid.nb.cm<-confusionMatrix(mynb.pred,valid.df$malignant,positive="1")
valid.nb.cm
model.stats[2,]=c("NaiveBayes",train.nb.cm$overall[1],valid.nb.cm$overall[1])
```

The validation data set predictions have almost the same accuracy at 97.5%. The model is now run on the full data set and the results are stored for use with the ensemble.

```{r naiveBayes_full_predict}
mynb.pred <- predict(mynb, df)
model.results$nb<-as.numeric(as.character(mynb.pred))
```

The next model is built with the Neural Network algorithm. This algorithm uses hidden nodes between the input and output nodes to simulate complex relationships. Unlike the previous models, this model must use numeric data. Additionally, it works best if the inputs are scaled to values between 0 and 1. Below the progam will create a duplicate set of data where the data will be converted from factors to numbers between 0 and 1.

```{r neuralnet_setup}
df2<-df
#convert to integers #There are better ways to do this, but this shows each step of the process.
df2$Cl.thickness<-as.integer(df$Cl.thickness)
df2$malignant<-ifelse(as.integer(df$malignant)==2,1,0)
df2$Mitoses<-as.integer(df$Mitoses)
df2$Cell.size<-as.integer(df$Cell.size)
df2$Cell.shape<-as.integer(df$Cell.shape)
df2$Marg.adhesion<-as.integer(df$Marg.adhesion)
df2$Epith.c.size<-as.integer(df$Epith.c.size)
df2$Bare.nuclei<-as.integer(df$Bare.nuclei)
df2$Bl.cromatin<-as.integer(df$Bl.cromatin)
df2$Normal.nucleoli<-as.integer(df$Normal.nucleoli)

preproc<-preProcess(df2,method = "range")  #convert numeric data to 0 to 1 range.
df.range<-predict(preproc,df2)

train.df.range<-df.range[train_ind,] #recreate the training and validation data with the new scaled data
valid.df.range<-df.range[-train_ind,]
```

Now that the data has been prepared, the Neural Network can be trained. This network will run through training 11 times. This is because neural networks are usually trained with random starting points. This can cause slight differences depending on the starting values. By picking the response that shows up in majority of the repetitions, it can help get a more consistent performance from the neural network.

```{r neuralnet_training}
mynn <- neuralnet::neuralnet(malignant ~ ., data=train.df.range,linear.output=T,hidden=c(5),rep=11)
plot(mynn)
mynn.pred<-matrix(0,nrow=dim(train.df.range)[1],ncol=11) #creating an empty matrix to store results for each run of neural network
for (n in seq(1,11)){
  mynn.pred[,n]<-ifelse(predict(mynn, newdata=train.df.range,rep=n)>.5,1,0) #set a cutoff of .5 so anything above will count as malignant
}
#get ready for ensemble model inception
mynn.pred.sum<-ifelse(rowSums(mynn.pred)>=6,1,0) #sum the 1s in each row and if majority of cases is 1, classify row as malignant
train.nn.cm<-confusionMatrix(as.factor(mynn.pred.sum),as.factor(train.df.range$malignant),positive = "1")
print(train.nn.cm$overall[1])
train.nn.cm
```

The training neural network has perfect accuracy. It is unlikely to remain that way when run on the validation data.

```{r neuralnet_valid}
mynn.pred<-matrix(0,nrow=dim(valid.df.range)[1],ncol=11)
for (n in seq(1,11)){
  mynn.pred[,n]<-ifelse(predict(mynn, newdata=valid.df.range,rep=n)>.5,1,0)
}
mynn.pred.sum<-ifelse(rowSums(mynn.pred)>=6,1,0)
valid.nn.cm<-confusionMatrix(as.factor(mynn.pred.sum),as.factor(valid.df.range$malignant),positive = "1")
valid.nn.cm
model.stats[3,]=c("Neural Network",train.nn.cm$overall[1],valid.nn.cm$overall[1])
```

The validation data model has a drop of accuracy to 95.6%. With validation complete, the model is run against the full data set. 

```{r neuralnet_full_predict}
mynn.pred<-matrix(0,nrow=dim(df.range)[1],ncol=11)
for (n in seq(1,11)){
  mynn.pred[,n]<-ifelse(predict(mynn, newdata=df.range,rep=n)>.5,1,0)
}
mynn.pred.sum<-ifelse(rowSums(mynn.pred)>=6,1,0)
model.results$nn<-as.numeric(as.character(mynn.pred.sum))
```

The next model uses a decision tree algorithm. The decision tree is built by finding the best point to divide data so that each division reduces the amount of impurity in the two sides of the data. The impurity can be measured in different ways such as the Gini index or entropy measure. The process is repeated on each new set of data until the branch reaches a single class or a certain number of records. 

```{r decision_tree_training}
mytree <- rpart(malignant ~ ., train.df)
plot(mytree); text(mytree) 
mytree.pred <- predict(mytree,newdata=train.df,type="class")
train.dt.cm<-confusionMatrix(as.factor(mytree.pred),as.factor(train.df$malignant),positive = "1")
train.dt.cm
```

The training data on the decision tree is already low when compared to the training data results of the earlier models.

```{r decision_tree_validation}
mytree.pred <- predict(mytree,newdata=valid.df,type="class")
valid.dt.cm<-confusionMatrix(as.factor(mytree.pred),as.factor(valid.df$malignant),positive = "1")
valid.dt.cm
model.stats[4,]=c("Decision Tree",train.dt.cm$overall[1],valid.dt.cm$overall[1])
```

The decision tree validation data results in a slightly lower accuracy of 94.2%. Now the model will be run against the full data set and the results will be saved for use with the ensemble model.

```{r}
mytree.pred <- predict(mytree,newdata=df,type="class")
model.results$dt<-as.numeric(as.character(mytree.pred))
```


Cross-validation decision trees work by removing one record from the data and using the remaining data as training. It then makes a prediction for the single record that wasn't included in training. 

Cross-Validation decision tree method doesn't have an easy way to split data into training and validation. Because of this, the full dataset is used rather than the model being run one for training and once for validation datasets.

```{r crossValidation}
ans <- numeric(length(df[,1]))
for (i in 1:length(df[,1])) {
  mytree <- rpart(malignant ~ ., df[-i,])
  mytree.pred <- predict(mytree,df[i,],type="class")
  ans[i] <- mytree.pred
}
ans <- factor(ans,labels=levels(df$malignant))

sv.cm<-confusionMatrix(as.factor(ans),as.factor(df$malignant),positive = "1")
sv.cm
model.stats[5,]=c("Cross Validation","",sv.cm$overall[1])

model.results$cv<-as.numeric(as.character(ans)) #store results for ensemble
```

Cross-validation gives a similar result to the decision tree in terms of accuracy. Since this model wasn't run separately on training and validation sets, we will just use the full results already calculated.

Quadratic Discriminant Analysis or QDA is the quadratic version of Linear Discriminant Analysis (LDA). In LDA, all data undergoes dimensional reduction to a line. A point is chosen on the line and all points for a given side are assigned a class. QDA builds on the LDA approach but allows for non-linear relationships between predictors. Interestly LDA/QDA make use of Bayes Theorum similar to Naive Bayes models.

```{r QDA_training}
#Quadratic Discriminant Analysis

train.df2<-df2[train_ind,]
valid.df2<-df2[-train_ind,]

myqda <- qda(malignant ~ ., data=train.df2)
myqda.pred <- predict(myqda, train.df2)
# table(myqda.pred$class,training.df2$malignant)
train.qda.cm<-confusionMatrix(as.factor(myqda.pred$class),as.factor(train.df2$malignant),positive = "1")
train.qda.cm
```

```{r QDA_validation}
#Quadratic Discriminant Analysis

myqda.pred <- predict(myqda, valid.df2)
# table(myqda.pred$class,training.df2$malignant)
valid.qda.cm<-confusionMatrix(as.factor(myqda.pred$class),as.factor(valid.df2$malignant),positive = "1")
valid.qda.cm
model.stats[6,]=c("Quadratic Discriminant Analysis",train.qda.cm$overall[1],valid.qda.cm$overall[1])
```

The accuracy of QDA for the training and validation data is similar with a score of 95.8% and 95.6% respectively.

```{r QDA_full_predict}
myqda.pred <- predict(myqda, df2)
model.results$qda<-as.numeric(as.character(myqda.pred$class))
```

RDA or Regularised Discriminant Analysis is a variant of QDA. The RDA method works well when there are a large number of features or if there is a lot of multicollinearity in the predictor data.

```{r RDA_training}
#Regularised Discriminant Analysis
myrda <- rda(malignant ~ ., train.df2)
myrda.pred <- predict(myrda, train.df2)
train.rda.cm<-confusionMatrix(as.factor(myrda.pred$class),as.factor(train.df2$malignant),positive = "1")
train.rda.cm
```

```{r RDA_valid}
#Regularised Discriminant Analysis
myrda.pred <- predict(myrda, valid.df2)
valid.rda.cm<-confusionMatrix(as.factor(myrda.pred$class),as.factor(valid.df2$malignant),positive = "1")
valid.rda.cm
model.stats[7,]=c("Regularised Discriminant Analysis",train.rda.cm$overall[1],valid.rda.cm$overall[1])
```

RDA outperforms QDA in both the training and validation data sets. The model is now run on the full data.

```{r}
myrda.pred <- predict(myrda, df2)
model.results$rda<-as.numeric(as.character(myrda.pred$class))
```

The Random Forest model uses a combination of multiple decision trees. Within each stage different subtrees are built and combined together using voting. This allows the random forest to improve its accuracy over a single decision tree model.

```{r rf_training}
#Random Forests
myrf <- randomForest(malignant ~ .,train.df)
myrf.pred <- predict(myrf, train.df)
train.rf.cm<-confusionMatrix(as.factor(myrf.pred),as.factor(train.df$malignant),positive = "1")
train.rf.cm
```

```{r rf_valid}
#Random Forests
myrf.pred <- predict(myrf, valid.df)
valid.rf.cm<-confusionMatrix(as.factor(myrf.pred),as.factor(valid.df$malignant),positive = "1")
valid.rf.cm
myrf.pred.comp <- predict(myrf, df)
complete.rf.cm<-confusionMatrix(as.factor(myrf.pred.comp),as.factor(df$malignant),positive = "1")
complete.rf.cm
model.stats[8,]=c("Random Forest",train.rf.cm$overall[1],valid.rf.cm$overall[1])
```

The Random Forest model gives improved accuracy over the decision tree model with accuracies of 100% and 97.1% in training and validation data. As with all the other models, the Random Forest Model is run on the full data set and the results are stored.

```{r}
myrf.pred <- predict(myrf, df)
model.results$rf<-as.numeric(as.character(myrf.pred))
```

Now that all the models have been run, it is time to create the ensemble model. This model will look at the classifications for each row of data and pick the majority classification. Since there are 8 models being used there is a chance of a tie. There are several possible ways to break a tie. I have chosen to use the results of the most accurate model according to validation data to break ties. In this case, Naive Bayes has the highest validation accuracy.

```{r calc_ensemble}
print(model.stats)
model.results$Row.Sum<-rowSums(model.results[,c(2,3,4,5,6,7,8,9)])
model.results$ensemble.pick<-ifelse(model.results$Row.Sum>4,1,ifelse(model.results$Row.Sum==4,model.results$rf,0))
```

Now that the ensemble predictor has been created, it is time to compare its accuracy against the individual models. In order to best gauge accuracy with possible new data, only validation data will be used to score each model.

```{r review_models}
ensemble.cm<-confusionMatrix(as.factor(model.results$ensemble.pick[-train_ind]),as.factor(model.results$Actual.Malignant[-train_ind]))
ensemble.cm
model.stats[9,]<-c("Ensemble Model","",ensemble.cm$overall[1])
model.stats$Misclassification.Count<-round((1-as.numeric(model.stats$Validation.Accuracy))*dim(valid.df)[1])
print(model.stats[order(model.stats$Validation.Accuracy,decreasing = TRUE),])

```

The ensemble model does very well and ties for second place with the Random Forest model.However, it might be possible to increase the accuracy further. The next step is to explore how the ensemble relates to the other models.

```{r}
cer.cor<-cor(model.results[c(2:9,11)])
plot(cer.cor)
heatmap.2(cer.cor, Rowv = FALSE, Colv = FALSE, dendrogram = "none", 
          cellnote = round(cer.cor,2), 
          notecol = "black", key = TRUE, trace = 'none', margins = c(5,5))
```

```{r decision}
correct.df<-as.data.frame(ifelse(model.results[,c(2:9,11)]==model.results$Actual.Malignant,1,0))
selected.df<-as.data.frame(ifelse(model.results[,c(2:9)]==model.results$ensemble.pick,1,0))
selected.df$correct<-ifelse(model.results$Actual.Malignant==model.results$ensemble.pick,"Yes","No")
selected.df$decision.cnt<-rowSums(selected.df[c(1:8)])
ggplot(data=selected.df,aes(x=decision.cnt,fill=correct))+geom_histogram(bins=5)+scale_x_continuous("Models in Agreement")+ guides(fill=guide_legend(title="Correct Classification"))
ggplot(data=selected.df,aes(x=decision.cnt,fill=factor(correct)))+geom_histogram(bins=5,position="fill")+scale_x_continuous("Models in Agreement")+ guides(fill=guide_legend(title="Correct Classification"))
```

```{r wrong_pick_analysis}

wrong.cnt<-dim(correct.df[correct.df$ensemble.pick==0,])[1]
wrong.df<-as.data.frame(colSums(correct.df[correct.df$ensemble.pick==0,c(1:8)]))
colnames(wrong.df)<-"Correct Count"
wrong.df$'Percent Correct'<-round(wrong.df$`Correct Count`/wrong.cnt,2)
print(wrong.df)
```

When we look at which model consistently vote wrong in the misclassified set, we see that the decision tree never goes against the ensemble to provide the correct answer. Let's drop the decision tree from the ensemble and see if that can increase the ensemble's accuracy. As an added bonus, this will make the number of models polled an odd number so there will no longer be a need for tie-breakers.

```{r calc_ensemble_2}
model.results$Row.Sum.B<-rowSums(model.results[,c(2,3,4,6,7,8,9)])
model.results$ensemble.pick.B<-ifelse(model.results$Row.Sum.B>=4,1,0)
ensemble.B.cm<-confusionMatrix(as.factor(model.results$ensemble.pick.B[-train_ind]),as.factor(model.results$Actual.Malignant[-train_ind]))
ensemble.B.cm
model.stats[10,]<-c("Ensemble Model B","",ensemble.B.cm$overall[1],round((1-as.numeric(ensemble.B.cm$overall[1]))*dim(valid.df)[1]))
print(model.stats[order(model.stats$Validation.Accuracy,decreasing = TRUE),])
```

The decision tree model was the least accurate models in the ensemble and it never went against the consensus pick for misclassifications. When it was dropped from the ensemble model, it boosted the accuracy from 0.971 to 0.974. That is the difference of one additional correct classification.